{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b72625-ca0e-4368-b3f0-be6938f59801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip conda install -c conda-forge pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0f9e6d-9566-4b2a-a8af-20b62c96e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3546ebe8-bcbb-4e31-86f4-9f4686c4ed45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>15.07</td>\n",
       "      <td>15.12</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.75</td>\n",
       "      <td>8407500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>14.89</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.26</td>\n",
       "      <td>14.46</td>\n",
       "      <td>8882000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>14.45</td>\n",
       "      <td>14.51</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.27</td>\n",
       "      <td>8126000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>14.30</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.25</td>\n",
       "      <td>14.66</td>\n",
       "      <td>10259500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.16</td>\n",
       "      <td>13.99</td>\n",
       "      <td>31879900</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   open   high    low  close    volume name\n",
       "0  2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
       "1  2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
       "2  2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
       "3  2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
       "4  2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = \"all_stocks_5yr.csv\"\n",
    "parquet_file_path = \"all_stocks_5yr.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68aca19-22d5-45fc-b160-3fd8b0db8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read/write times benchmarking\n",
    "\n",
    "def benchmark_storage(df, scale_factor):\n",
    "    print(f\"\\nðŸš€ Benchmarking for {scale_factor}x dataset...\\n\")\n",
    "\n",
    "    # scale dataset\n",
    "    df_scaled = pd.concat([df] * scale_factor, ignore_index=True)\n",
    "\n",
    "    results = {\n",
    "        \"Scale\": scale_factor,\n",
    "        \"CSV Read Time (s)\": None,\n",
    "        \"CSV Write Time (s)\": None,\n",
    "        \"CSV Size (MB)\": None,\n",
    "        \"Parquet Read Time (s)\": None,\n",
    "        \"Parquet Write Time (s)\": None,\n",
    "        \"Parquet Size (MB)\": None\n",
    "    }\n",
    "\n",
    "    # optimized CSV handling for 100x\n",
    "    csv_scaled_path = f\"stocks_{scale_factor}x.csv\"\n",
    "\n",
    "    # measure csv write time (using chunks for efficiency)\n",
    "    start_time = time.time()\n",
    "    df_scaled.to_csv(csv_scaled_path, index=False, chunksize=1000000)  # larger chunk size for 100x\n",
    "    results[\"CSV Write Time (s)\"] = time.time() - start_time\n",
    "    print(f\"CSV written: {csv_scaled_path} in {results['CSV Write Time (s)']:.2f} seconds\")\n",
    "\n",
    "    # measure CSV read time\n",
    "    start_time = time.time()\n",
    "    pd.read_csv(csv_scaled_path)\n",
    "    results[\"CSV Read Time (s)\"] = time.time() - start_time\n",
    "    print(f\"ðŸ“– CSV read completed in {results['CSV Read Time (s)']:.2f} seconds\")\n",
    "    \n",
    "    # measure CSV file size\n",
    "    results[\"CSV Size (MB)\"] = os.path.getsize(csv_scaled_path) / (1024 * 1024)\n",
    "    print(f\"ðŸ“‚ CSV file size: {results['CSV Size (MB)']:.2f} MB\")\n",
    "\n",
    "    # write parquet for all scales\n",
    "    parquet_scaled_path = f\"stocks_{scale_factor}x.parquet\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_scaled.to_parquet(parquet_scaled_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    results[\"Parquet Write Time (s)\"] = time.time() - start_time\n",
    "    print(f\" Parquet written: {parquet_scaled_path} in {results['Parquet Write Time (s)']:.2f} seconds\")\n",
    "\n",
    "    # measure parquet read time\n",
    "    start_time = time.time()\n",
    "    pd.read_parquet(parquet_scaled_path, engine=\"pyarrow\")\n",
    "    results[\"Parquet Read Time (s)\"] = time.time() - start_time\n",
    "    print(f\"Parquet read completed in {results['Parquet Read Time (s)']:.2f} seconds\")\n",
    "\n",
    "    # measure parquet file size\n",
    "    results[\"Parquet Size (MB)\"] = os.path.getsize(parquet_scaled_path) / (1024 * 1024)\n",
    "    print(f\" Parquet file size: {results['Parquet Size (MB)']:.2f} MB\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03caf3c1-e10c-4c45-a1a2-0aef107ebb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Benchmarking for 1x dataset...\n",
      "\n",
      "CSV written: stocks_1x.csv in 2.53 seconds\n",
      "ðŸ“– CSV read completed in 0.38 seconds\n",
      "ðŸ“‚ CSV file size: 28.21 MB\n",
      " Parquet written: stocks_1x.parquet in 0.25 seconds\n",
      "Parquet read completed in 0.18 seconds\n",
      " Parquet file size: 10.15 MB\n",
      "\n",
      "ðŸš€ Benchmarking for 10x dataset...\n",
      "\n",
      "CSV written: stocks_10x.csv in 23.66 seconds\n",
      "ðŸ“– CSV read completed in 3.65 seconds\n",
      "ðŸ“‚ CSV file size: 282.10 MB\n",
      " Parquet written: stocks_10x.parquet in 1.83 seconds\n",
      "Parquet read completed in 0.57 seconds\n",
      " Parquet file size: 95.35 MB\n",
      "\n",
      "ðŸš€ Benchmarking for 100x dataset...\n",
      "\n",
      "CSV written: stocks_100x.csv in 242.63 seconds\n",
      "ðŸ“– CSV read completed in 59.86 seconds\n",
      "ðŸ“‚ CSV file size: 2821.02 MB\n",
      " Parquet written: stocks_100x.parquet in 28.03 seconds\n",
      "Parquet read completed in 23.60 seconds\n",
      " Parquet file size: 951.71 MB\n"
     ]
    }
   ],
   "source": [
    "# run benchmarks for 1x, 10x, and 100x\n",
    "benchmark_results = []\n",
    "for scale in [1, 10, 100]:  # now fully benchmarks CSV & parquex at 100x\n",
    "    benchmark_results.append(benchmark_storage(df, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892a61e5-4fe4-4991-8a33-b2a4ee320d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking Results:\n",
      "\n",
      "   Scale  CSV Read Time (s)  CSV Write Time (s)  CSV Size (MB)  \\\n",
      "0      1           0.383850            2.526141      28.210210   \n",
      "1     10           3.646927           23.659400     282.101781   \n",
      "2    100          59.858686          242.633004    2821.017491   \n",
      "\n",
      "   Parquet Read Time (s)  Parquet Write Time (s)  Parquet Size (MB)  \n",
      "0               0.182963                0.249073          10.151486  \n",
      "1               0.574579                1.833106          95.354862  \n",
      "2              23.599433               28.027754         951.709558  \n"
     ]
    }
   ],
   "source": [
    "# convert to df and print\n",
    "\n",
    "df_results = pd.DataFrame(benchmark_results)\n",
    "print(\"\\nBenchmarking Results:\\n\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2bac81-70cd-41b6-a748-27bd5e4f5305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking Results:\n",
      "\n",
      "   Scale  CSV Read Time (s)  CSV Write Time (s)  CSV Size (MB)  \\\n",
      "0      1           0.383850            2.526141      28.210210   \n",
      "1     10           3.646927           23.659400     282.101781   \n",
      "2    100          59.858686          242.633004    2821.017491   \n",
      "\n",
      "   Parquet Read Time (s)  Parquet Write Time (s)  Parquet Size (MB)  \n",
      "0               0.182963                0.249073          10.151486  \n",
      "1               0.574579                1.833106          95.354862  \n",
      "2              23.599433               28.027754         951.709558  \n"
     ]
    }
   ],
   "source": [
    "#save to csv\n",
    "\n",
    "df_results = pd.DataFrame(benchmark_results)\n",
    "print(\"\\nBenchmarking Results:\\n\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1e219-6fd2-404c-b2fd-12449d325cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
